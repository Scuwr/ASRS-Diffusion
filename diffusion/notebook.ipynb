{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3500/3500 [2:01:51<00:00,  2.09s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2919 files retrieved in the \"train\" split.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [53:51<00:00,  3.59s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "710 files retrieved in the \"validation\" split.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import io\n",
    "import urllib\n",
    "from matplotlib import pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from datasets.utils.file_utils import get_datasets_user_agent\n",
    "import PIL.Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "USER_AGENT = get_datasets_user_agent()\n",
    "\n",
    "dir_path = \"dataset\"\n",
    "images_dir = \"images\"\n",
    "filepath_label = 'filepath'\n",
    "\n",
    "train_num = 3500 # Expect 15% of these images to not download\n",
    "valid_num = 900  # Expect 15% of these images to not download\n",
    "\n",
    "def get_img(url, retries=2):\n",
    "    for _ in range(retries + 1):\n",
    "            try:\n",
    "                request = urllib.request.Request(url, data=None, headers={'user-agent': USER_AGENT})\n",
    "                with urllib.request.urlopen(request) as req:\n",
    "                    image = PIL.Image.open(io.BytesIO(req.read()))\n",
    "                break\n",
    "            except:\n",
    "                image = None\n",
    "    \n",
    "    return image\n",
    "\n",
    "def load_data(key, df, num):\n",
    "    count = 0\n",
    "    for i in tqdm(range(num)):\n",
    "        if not df[key][filepath_label][i]:\n",
    "            pass\n",
    "\n",
    "        elif df[key][filepath_label][i] == \"null\":\n",
    "            url = df[key]['image_url'][i]\n",
    "\n",
    "            image = get_img(url)\n",
    "            if not image is None:\n",
    "                filepath = os.path.join(dir_path, images_dir, f'{key}_{i}.jpg')\n",
    "                try:\n",
    "                    image.save(filepath)\n",
    "                    df[key][filepath_label][i] = filepath\n",
    "                    count += 1\n",
    "                except:\n",
    "                    df[key][filepath_label][i] = None\n",
    "            else:\n",
    "                df[key][filepath_label][i] = None\n",
    "                #print(f'Could not fetch index {i}!')\n",
    "\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "    print(f'{count} files retrieved in the \"{key}\" split.')\n",
    "    return df\n",
    "\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "    os.makedirs(os.path.join(dir_path, images_dir))\n",
    "\n",
    "    dset = load_dataset(\"conceptual_captions\")\n",
    "\n",
    "    filepaths = [\"null\"] * len(dset['train'])\n",
    "    dset['train'] = dset['train'].add_column(filepath_label, filepaths)\n",
    "\n",
    "    filepaths = [\"null\"] * len(dset['validation'])\n",
    "    dset['validation'] = dset['validation'].add_column(filepath_label, filepaths)\n",
    "\n",
    "    df_train = dset['train'].to_pandas()\n",
    "    df_valid = dset['validation'].to_pandas()\n",
    "\n",
    "    df = {\n",
    "        'train': df_train,\n",
    "        'validation': df_valid\n",
    "          }\n",
    "\n",
    "    with open(os.path.join(dir_path, 'df.pkl'), 'wb') as handle:\n",
    "        pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(dir_path, 'df.pkl'), 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "\n",
    "df = load_data('train', df, train_num)\n",
    "df = load_data('validation', df, valid_num)\n",
    "\n",
    "with open(os.path.join(dir_path, 'df.pkl'), 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#image = transforms.Compose([transforms.ToTensor(), utils.Rescale(64)])(image)\n",
    "#plt.figure(figsize=(8, 8))\n",
    "#plt.imshow(image.permute(1, 2, 0))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/archives/fgvc-aircraft-2013b.tar.gz to aircraft/fgvc-aircraft-2013b.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f29bd799eb4352bc5dcce90568566a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2753340328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting aircraft/fgvc-aircraft-2013b.tar.gz to aircraft\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset FGVCAircraft\n",
       "    Number of datapoints: 6667\n",
       "    Root location: aircraft"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import io\n",
    "import urllib\n",
    "from matplotlib import pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from datasets.utils.file_utils import get_datasets_user_agent\n",
    "import PIL.Image\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "dir_path = 'aircraft'\n",
    "\n",
    "datasets.FGVCAircraft(dir_path, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-small were not used when initializing T5EncoderModel: ['decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.final_layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 100,   19,    3,    9, 7142,   11,  541,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 8, 512])\n",
      "tensor([[True, True, True, True, True, True, True, True]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "\n",
    "text = [\"This is a sentence and again\"]\n",
    "max_length = 64\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', model_max_length=max_length)\n",
    "encoder = T5EncoderModel.from_pretrained('t5-small')\n",
    "\n",
    "device = torch.device('cuda')\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "tokenized = tokenizer.batch_encode_plus(text, padding='longest', max_length=max_length, \n",
    "                                        truncation=True, return_tensors='pt')\n",
    "\n",
    "print(tokenized)\n",
    "\n",
    "input_ids = tokenized.input_ids.to(device)\n",
    "attention_mask = tokenized.attention_mask.to(device)\n",
    "\n",
    "encoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    t5_out = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    final_encoding = t5_out.last_hidden_state.detach()\n",
    "\n",
    "final_encoding = final_encoding.masked_fill(~attention_mask.unsqueeze(2).bool(), 0.)\n",
    "\n",
    "print(final_encoding.shape)\n",
    "print(attention_mask.bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ldm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m autocast\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstablediffusion\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mldm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdiffusion\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mddim\u001b[39;00m \u001b[39mimport\u001b[39;00m DDIMSampler\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstablediffusion\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mldm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdiffusion\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplms\u001b[39;00m \u001b[39mimport\u001b[39;00m PLMSSampler\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstablediffusion\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mscripts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtxt2img\u001b[39;00m \u001b[39mimport\u001b[39;00m load_model_from_config\n",
      "File \u001b[0;32m/storage/cmarnold/ASRS-Diffusion/stablediffusion/ldm/models/diffusion/ddim.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m partial\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mldm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdiffusionmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m make_ddim_sampling_parameters, make_ddim_timesteps, noise_like, \\\n\u001b[1;32m      9\u001b[0m     extract_into_tensor\n\u001b[1;32m     12\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDDIMSampler\u001b[39;00m(\u001b[39mobject\u001b[39m):\n\u001b[1;32m     13\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model, schedule\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ldm'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import autocast\n",
    "\n",
    "from stablediffusion.ldm.models.diffusion.ddim import DDIMSampler\n",
    "from stablediffusion.ldm.models.diffusion.plms import PLMSSampler\n",
    "from stablediffusion.scripts.txt2img import load_model_from_config\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "prompt_i = \"a painting of a virus monster playing guitar\"\n",
    "prompt_j = \"a forested landscape\"\n",
    "\n",
    "w_i = 0.5\n",
    "w_j = 0.5\n",
    "\n",
    "config = \"../stablediffusion/configs/latent-diffusion/txt2img-1p4B-eval.yaml\"\n",
    "\n",
    "timesteps = 1000\n",
    "\n",
    "n = 1 # Number of samples / batch size\n",
    "ch = 4 # Latent channels\n",
    "f = 8 # Downsample factor\n",
    "h = 512 # Image height\n",
    "w = 512 # Image width\n",
    "\n",
    "scale = 7.5 # Unconditional guidance scale\n",
    "ddim_eta = 0.0 # 0.0 corresponds to deterministic sampling\n",
    "shape = [ch, h // f, w // f]\n",
    "\n",
    "b = n\n",
    "\n",
    "model = load_model_from_config(config, '../sd-v2-1.ckpt')\n",
    "model = model.to(device)\n",
    "model = PLMSSampler(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with autocast('cuda'):\n",
    "        with model.ema_scope():\n",
    "            uc = model.get_learned_conditioning(n * [\"\"])\n",
    "            c_i = model.get_learned_conditioning(n * [prompt_i])\n",
    "            c_j = model.get_learned_conditioning(n * [prompt_j])\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample(model, x, c, ts, index, old_eps=None, t_next=None):\n",
    "    outs = model.p_sample_plms(x, c, ts, index=index, unconditional_guidance_scale=scale, unconditional_conditioning=uc,)\n",
    "    x, _, e_t = outs\n",
    "    old_eps.append(e_t)\n",
    "    if len(old_eps) >= 4:\n",
    "        old_eps.pop(0)\n",
    "\n",
    "    return old_eps \n",
    "\n",
    "with torch.no_grad():\n",
    "    with autocast('cuda'):\n",
    "        with model.ema_scope():\n",
    "            # Initialize sample x_T to N(0,I)\n",
    "            x = torch.randn((n, ch, h // f, w // f)).to(device)\n",
    "\n",
    "            model.make_schedule(ddim_num_steps=timesteps, ddim_eta=ddim_eta, verbose=False)\n",
    "            timesteps = model.ddim_timesteps\n",
    "            time_range = np.flip(timesteps)\n",
    "            total_steps = timesteps.shape[0]\n",
    "            e_ti = []\n",
    "            e_tj = []\n",
    "            for i, step in enumerate(tqdm(time_range, desc='PLMS Sampler', total=total_steps)):\n",
    "                index = total_steps - i - 1\n",
    "                ts = torch.full((b,), step, device=device, dtype=torch.long)\n",
    "                ts_next = torch.full((b,), time_range[min(i + 1, len(time_range) - 1)], device=device, dtype=torch.long)\n",
    "                \n",
    "                # Compute conditional scores for each concept c_i\n",
    "                e_ti = p_sample(model, x, c_i, ts, index, e_ti, ts_next) \n",
    "                e_tj = p_sample(model, x, c_j, ts, index, e_tj, ts_next)\n",
    "                e_i = e_ti[-1]\n",
    "                e_j = e_tj[-1]\n",
    "\n",
    "\n",
    "                # Compute unconditional score\n",
    "                e_t = p_sample(model, x, uc, ts, index, e_t, ts_next)\n",
    "                e = e_t[-1]\n",
    "                \n",
    "                # Sampling\n",
    "                mean = x - (e + w_i * (e_i - e) + w_j * (e_j - e))\n",
    "                covar = model.betas[ts]\n",
    "                x = torch.normal(mean, covar*torch.eye(h // f, w // f)) # Sampling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
