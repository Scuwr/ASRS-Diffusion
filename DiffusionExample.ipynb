{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec77a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(PARAMETERS=None, NUM_WORKERS=0, BATCH_SIZE=1, MAX_NUM_WORDS=64, IMG_SIDE_LEN=128, EPOCHS=5, T5_NAME='t5_base', TRAIN_VALID_FRAC=0.9, TIMESTEPS=25, OPTIM_LR=0.0001, ACCUM_ITER=1, CHCKPT_NUM=500, VALID_NUM=None, RESTART_DIRECTORY=None, TESTING=True, timestamp=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: conceptual_captions/unlabeled\n",
      "Found cached dataset conceptual_captions (/Users/christian/.cache/huggingface/datasets/conceptual_captions/unlabeled/1.0.0/05266784888422e36944016874c44639bccb39069c2227435168ad8b02d600d8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f15a997d3e14b4b9d59ea075284bbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "betas:\n",
      "\ttensor([0.0040, 0.0372, 0.0703, 0.1035, 0.1367, 0.1698, 0.2030, 0.2362, 0.2693,\n",
      "        0.3025, 0.3357, 0.3688, 0.4020, 0.4352, 0.4683, 0.5015, 0.5347, 0.5678,\n",
      "        0.6010, 0.6342, 0.6673, 0.7005, 0.7337, 0.7668, 0.8000],\n",
      "       dtype=torch.float64)\n",
      "alphas_cumprod:\n",
      "\ttensor([9.9600e-01, 9.5898e-01, 8.9153e-01, 7.9926e-01, 6.9003e-01, 5.7284e-01,\n",
      "        4.5655e-01, 3.4873e-01, 2.5481e-01, 1.7773e-01, 1.1807e-01, 7.4522e-02,\n",
      "        4.4564e-02, 2.5171e-02, 1.3383e-02, 6.6713e-03, 3.1044e-03, 1.3416e-03,\n",
      "        5.3530e-04, 1.9583e-04, 6.5146e-05, 1.9511e-05, 5.1965e-06, 1.2117e-06,\n",
      "        2.4233e-07], dtype=torch.float64)\n",
      "betas:\n",
      "\ttensor([0.0040, 0.0372, 0.0703, 0.1035, 0.1367, 0.1698, 0.2030, 0.2362, 0.2693,\n",
      "        0.3025, 0.3357, 0.3688, 0.4020, 0.4352, 0.4683, 0.5015, 0.5347, 0.5678,\n",
      "        0.6010, 0.6342, 0.6673, 0.7005, 0.7337, 0.7668, 0.8000],\n",
      "       dtype=torch.float64)\n",
      "alphas_cumprod:\n",
      "\ttensor([9.9600e-01, 9.5898e-01, 8.9153e-01, 7.9926e-01, 6.9003e-01, 5.7284e-01,\n",
      "        4.5655e-01, 3.4873e-01, 2.5481e-01, 1.7773e-01, 1.1807e-01, 7.4522e-02,\n",
      "        4.4564e-02, 2.5171e-02, 1.3383e-02, 6.6713e-03, 3.1044e-03, 1.3416e-03,\n",
      "        5.3530e-04, 1.9583e-04, 6.5146e-05, 1.9511e-05, 5.1965e-06, 1.2117e-06,\n",
      "        2.4233e-07], dtype=torch.float64)\n",
      "betas:\n",
      "\ttensor([0.0040, 0.0372, 0.0703, 0.1035, 0.1367, 0.1698, 0.2030, 0.2362, 0.2693,\n",
      "        0.3025, 0.3357, 0.3688, 0.4020, 0.4352, 0.4683, 0.5015, 0.5347, 0.5678,\n",
      "        0.6010, 0.6342, 0.6673, 0.7005, 0.7337, 0.7668, 0.8000],\n",
      "       dtype=torch.float64)\n",
      "alphas_cumprod:\n",
      "\ttensor([9.9600e-01, 9.5898e-01, 8.9153e-01, 7.9926e-01, 6.9003e-01, 5.7284e-01,\n",
      "        4.5655e-01, 3.4873e-01, 2.5481e-01, 1.7773e-01, 1.1807e-01, 7.4522e-02,\n",
      "        4.4564e-02, 2.5171e-02, 1.3383e-02, 6.6713e-03, 3.1044e-03, 1.3416e-03,\n",
      "        5.3530e-04, 1.9583e-04, 6.5146e-05, 1.9511e-05, 5.1965e-06, 1.2117e-06,\n",
      "        2.4233e-07], dtype=torch.float64)\n",
      "\n",
      "-------------------- EPOCH 1 --------------------\n",
      "\n",
      "----------Training...----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a979f13e2342ffbefa4601a009ed5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1125fa42f0f494ba083170627134c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f68e6ae2ffb4162848b46f2dff01feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-small were not used when initializing T5EncoderModel: ['decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.layer_norm.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "0it [00:20, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GaussianDiffusion' object has no attribute '_sample_random_times'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/ASRS-Diffusion/training.py:398\u001b[0m, in \u001b[0;36mMinimagenTrain\u001b[0;34m(timestamp, args, unets, imagen, train_dataloader, valid_dataloader, training_dir, optimizer, timeout)\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m         train()\n\u001b[1;32m    399\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m    400\u001b[0m     \u001b[39m# If batch is empty, move on to the next one\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ASRS-Diffusion/training.py:311\u001b[0m, in \u001b[0;36mMinimagenTrain.<locals>.train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mfor\u001b[39;00m unet_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(unets)):\n\u001b[0;32m--> 311\u001b[0m     loss \u001b[39m=\u001b[39m imagen(images, text_embeds\u001b[39m=\u001b[39;49mencoding, text_masks\u001b[39m=\u001b[39;49mmask, unet_number\u001b[39m=\u001b[39;49munet_idx \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    312\u001b[0m     losses[unet_idx] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ASRS-Diffusion/Imagen.py:395\u001b[0m, in \u001b[0;36mImagen.forward\u001b[0;34m(self, images, texts, text_embeds, text_masks, unet_number)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39m# Randomly sample a timestep value for each image in the batch.\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m times \u001b[39m=\u001b[39m noise_scheduler\u001b[39m.\u001b[39;49m_sample_random_times(b, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    397\u001b[0m \u001b[39m# If text conditioning info supplied as text rather than embeddings, calculate the embeddings/mask\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GaussianDiffusion' object has no attribute '_sample_random_times'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(imagen\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mOPTIM_LR)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Train the MinImagen instance\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[43mMinimagenTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimagen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ASRS-Diffusion/training.py:404\u001b[0m, in \u001b[0;36mMinimagenTrain\u001b[0;34m(timestamp, args, unets, imagen, train_dataloader, valid_dataloader, training_dir, optimizer, timeout)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batch:\n\u001b[1;32m    402\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m     train()\n\u001b[1;32m    405\u001b[0m \u001b[39m# If batch takes longer than `timeout`, go onto the next\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39mexcept\u001b[39;00m _Timeout\u001b[39m.\u001b[39m_Timeout:\n",
      "File \u001b[0;32m~/Documents/ASRS-Diffusion/training.py:311\u001b[0m, in \u001b[0;36mMinimagenTrain.<locals>.train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m losses \u001b[39m=\u001b[39m [\u001b[39m0.\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(unets))]\n\u001b[1;32m    310\u001b[0m \u001b[39mfor\u001b[39;00m unet_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(unets)):\n\u001b[0;32m--> 311\u001b[0m     loss \u001b[39m=\u001b[39m imagen(images, text_embeds\u001b[39m=\u001b[39;49mencoding, text_masks\u001b[39m=\u001b[39;49mmask, unet_number\u001b[39m=\u001b[39;49munet_idx \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    312\u001b[0m     losses[unet_idx] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m    313\u001b[0m     running_train_loss[unet_idx] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/ASRS-Diffusion/Imagen.py:395\u001b[0m, in \u001b[0;36mImagen.forward\u001b[0;34m(self, images, texts, text_embeds, text_masks, unet_number)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39massert\u001b[39;00m h \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m target_image_size \u001b[39mand\u001b[39;00m w \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m target_image_size\n\u001b[1;32m    394\u001b[0m \u001b[39m# Randomly sample a timestep value for each image in the batch.\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m times \u001b[39m=\u001b[39m noise_scheduler\u001b[39m.\u001b[39;49m_sample_random_times(b, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    397\u001b[0m \u001b[39m# If text conditioning info supplied as text rather than embeddings, calculate the embeddings/mask\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[39mif\u001b[39;00m exists(texts) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m exists(text_embeds):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GaussianDiffusion' object has no attribute '_sample_random_times'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "\n",
    "from Imagen import Imagen\n",
    "from Unet import Unet, Base, Super, BaseTest, SuperTest\n",
    "from generate import load_minimagen, load_params\n",
    "from t5 import get_encoded_dim\n",
    "from training import get_minimagen_parser, ConceptualCaptions, get_minimagen_dl_opts, \\\n",
    "    create_directory, get_model_params, get_model_size, save_training_info, get_default_args, MinimagenTrain, \\\n",
    "    load_restart_training_parameters, load_testing_parameters\n",
    "\n",
    "# Get device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Command line argument parser. See `training.get_minimagen_parser()`.\n",
    "parser = get_minimagen_parser()\n",
    "# Add argument for when using `main.py`\n",
    "parser.add_argument(\"-ts\", \"--TIMESTAMP\", dest=\"timestamp\", help=\"Timestamp for training directory\", type=str,\n",
    "                             default=None)\n",
    "args = parser.parse_args([\"-b\", \"1\", \"-t\", \"25\", \"-test\"])\n",
    "print(args)\n",
    "timestamp = args.timestamp\n",
    "\n",
    "# Get training timestamp for when running train.py as main rather than via main.py\n",
    "if timestamp is None:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create training directory\n",
    "dir_path = f\"./training_{timestamp}\"\n",
    "training_dir = create_directory(dir_path)\n",
    "\n",
    "# If loading from a parameters/training directory\n",
    "if args.RESTART_DIRECTORY is not None:\n",
    "    args = load_restart_training_parameters(args)\n",
    "elif args.PARAMETERS is not None:\n",
    "    args = load_restart_training_parameters(args, justparams=True)\n",
    "\n",
    "# If testing, lower parameter values to lower computational load and also to lower amount of data being used.\n",
    "if args.TESTING:\n",
    "    args = load_testing_parameters(args)\n",
    "    train_dataset, valid_dataset = ConceptualCaptions(args, smalldata=True)\n",
    "else:\n",
    "    train_dataset, valid_dataset = ConceptualCaptions(args, smalldata=False)\n",
    "\n",
    "# Create dataloaders\n",
    "dl_opts = {**get_minimagen_dl_opts(device), 'batch_size': args.BATCH_SIZE, 'num_workers': args.NUM_WORKERS}\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, **dl_opts)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, **dl_opts)\n",
    "\n",
    "# Create Unets\n",
    "if args.RESTART_DIRECTORY is None:\n",
    "    imagen_params = dict(\n",
    "        image_sizes=(int(args.IMG_SIDE_LEN / 2), args.IMG_SIDE_LEN),\n",
    "        timesteps=args.TIMESTEPS,\n",
    "        cond_drop_prob=0.15,\n",
    "        text_encoder_name=args.T5_NAME\n",
    "    )\n",
    "\n",
    "    # If not loading a training from a checkpoint\n",
    "    if args.TESTING:\n",
    "        # If testing, use tiny MinImagen for low computational load\n",
    "        unets_params = [get_default_args(BaseTest), get_default_args(SuperTest)]\n",
    "\n",
    "    # Else if not loading Unet/Imagen settings from a config (parameters) folder, use defaults\n",
    "    elif not args.PARAMETERS:\n",
    "        # If no parameters provided, use params from minimagen.Imagen.Base and minimagen.Imagen.Super built-in classes\n",
    "        unets_params = [get_default_args(Base), get_default_args(Super)]\n",
    "\n",
    "    # Else load unet/Imagen configs from config (parameters) folder (override imagen+params)\n",
    "    else:\n",
    "        # If parameters are provided, load them\n",
    "        unets_params, imagen_params = get_model_params(args.PARAMETERS)\n",
    "\n",
    "    # Create Unets accoridng to unets_params\n",
    "    unets = [Unet(**unet_params).to(device) for unet_params in unets_params]\n",
    "\n",
    "    # Create Imagen from UNets with specified imagen parameters\n",
    "    imagen = Imagen(unets=unets, **imagen_params).to(device)\n",
    "else:\n",
    "    # If training is being resumed from a previous one, load all relevant models/info (load config AND state dicts)\n",
    "    orig_train_dir = os.path.join(os.getcwd(), args.RESTART_DIRECTORY)\n",
    "    unets_params, imagen_params = load_params(orig_train_dir)\n",
    "    imagen = load_minimagen(orig_train_dir).to(device)\n",
    "    unets = imagen.unets\n",
    "\n",
    "# Fill in unspecified arguments with defaults for complete config (parameters) file\n",
    "unets_params = [{**get_default_args(Unet), **i} for i in unets_params]\n",
    "imagen_params = {**get_default_args(Imagen), **imagen_params}\n",
    "\n",
    "# Get the size of the Imagen model in megabytes\n",
    "model_size_MB = get_model_size(imagen)\n",
    "\n",
    "# Save all training info (config files, model size, etc.)\n",
    "save_training_info(args, timestamp, unets_params, imagen_params, model_size_MB, training_dir)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.Adam(imagen.parameters(), lr=args.OPTIM_LR)\n",
    "\n",
    "# Train the MinImagen instance\n",
    "MinimagenTrain(timestamp, args, unets, imagen, train_dataloader, valid_dataloader, training_dir, optimizer, timeout=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cf3a2657c4530458a1d5b90a9ba637718c74089d900d5938397f33b4197fc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
