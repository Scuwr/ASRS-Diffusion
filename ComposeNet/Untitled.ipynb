{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "082baa53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./models/control_sd21_ini.ckpt\n",
      "No module 'xformers'. Proceeding without it.\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 118\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mema_scope():\n\u001b[1;32m    117\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 118\u001b[0m     uc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_learned_conditioning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    119\u001b[0m     c_i \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_learned_conditioning(n \u001b[38;5;241m*\u001b[39m [prompt_i])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    120\u001b[0m     c_j \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_learned_conditioning(n \u001b[38;5;241m*\u001b[39m [prompt_j])\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/storage/cmarnold/ASRS-Diffusion/ComposeNet/ldm/models/diffusion/ddpm.py:667\u001b[0m, in \u001b[0;36mLatentDiffusion.get_learned_conditioning\u001b[0;34m(self, c)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_stage_forward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_stage_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencode\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_stage_model\u001b[38;5;241m.\u001b[39mencode):\n\u001b[0;32m--> 667\u001b[0m         c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcond_stage_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, DiagonalGaussianDistribution):\n\u001b[1;32m    669\u001b[0m             c \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mmode()\n",
      "File \u001b[0;32m/storage/cmarnold/ASRS-Diffusion/ComposeNet/ldm/modules/encoders/modules.py:193\u001b[0m, in \u001b[0;36mFrozenOpenCLIPEmbedder.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/cmarnold/miniconda/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/storage/cmarnold/ASRS-Diffusion/ComposeNet/ldm/modules/encoders/modules.py:170\u001b[0m, in \u001b[0;36mFrozenOpenCLIPEmbedder.forward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m    169\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[0;32m--> 170\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_with_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m/storage/cmarnold/ASRS-Diffusion/ComposeNet/ldm/modules/encoders/modules.py:174\u001b[0m, in \u001b[0;36mFrozenOpenCLIPEmbedder.encode_with_transformer\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_with_transformer\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m--> 174\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, n_ctx, d_model]\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpositional_embedding\n\u001b[1;32m    176\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n",
      "File \u001b[0;32m/storage/cmarnold/miniconda/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/storage/cmarnold/miniconda/envs/control/lib/python3.8/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/cmarnold/miniconda/envs/control/lib/python3.8/site-packages/torch/nn/functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch import autocast\n",
    "from einops import rearrange\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import CircleDataset\n",
    "from cldm.util import load_model_from_config\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "\n",
    "config_path = './models/cldm_v21.yaml'\n",
    "config_path = './models/cldm_v21.yaml'\n",
    "model_path =  './models/CircleDataset_sd21_gs-001000.ckpt'\n",
    "model_path = './models/control_sd21_ini.ckpt'\n",
    "output_path = './testsamples'\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "\n",
    "prompt_i = \"blue circle with snow background\"\n",
    "prompt_j = \"Hogwarts\"\n",
    "\n",
    "w_i = 0.00\n",
    "w_j = 1.00\n",
    "\n",
    "n = 1 # Number of samples / batch size\n",
    "b = n \n",
    "ch = 4 # Latent channels\n",
    "f = 8 # Downsample factor\n",
    "h = 512 # Image height\n",
    "w = 512 # Image width\n",
    "\n",
    "scale = 7.5 # Unconditional guidance scale\n",
    "ddim_eta = 0.0 # 0.0 corresponds to deterministic sampling\n",
    "shape = [ch, h // f, w // f]\n",
    "\n",
    "\n",
    "# Create Control Images\n",
    "dataset = CircleDataset()\n",
    "control_i = torch.from_numpy((dataset[21]['hint'])[None, :])\n",
    "control_i = control_i.to(device)\n",
    "control_i = rearrange(control_i, 'b h w c -> b c h w')\n",
    "control_i = control_i.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "# Unconditioned Control Image\n",
    "control_j = torch.zeros_like(control_i)\n",
    "control_j = control_j.to(device)\n",
    "control_j = control_j.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "control_u = control_j\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = load_model_from_config(config_path, model_path)\n",
    "model = model.to(device)\n",
    "sampler = PLMSSampler(model)\n",
    "sampler.make_schedule(ddim_num_steps=50, ddim_eta=ddim_eta, verbose=False)\n",
    "\n",
    "# Get scaling factors from Sampler Schedule\n",
    "alphas = sampler.ddim_alphas\n",
    "alphas_prev = sampler.ddim_alphas_prev\n",
    "sqrt_one_minus_alphas = sampler.ddim_sqrt_one_minus_alphas\n",
    "sigmas = sampler.ddim_sigmas\n",
    "\n",
    "\n",
    "# Sampler Functions\n",
    "@torch.no_grad()\n",
    "def p_sample(model, x, c, ts, index, old_eps=None, t_next=None):\n",
    "    x, _, e_t = model.p_sample_plms(x, c, ts, index=index, unconditional_guidance_scale=scale, \n",
    "                                    unconditional_conditioning=uc, old_eps=old_eps, t_next=t_next)\n",
    "    old_eps.append(e_t)\n",
    "    if len(old_eps) >= 4:\n",
    "        old_eps.pop(0)\n",
    "\n",
    "    return x, old_eps \n",
    "\n",
    "def sample_x(x, e_t, index):\n",
    "    # select parameters corresponding to the currently considered timestep\n",
    "    a_t = torch.full((b, 1, 1, 1), alphas[index], device=device)\n",
    "    a_prev = torch.full((b, 1, 1, 1), alphas_prev[index], device=device)\n",
    "    sigma_t = torch.full((b, 1, 1, 1), sigmas[index], device=device)\n",
    "    sqrt_one_minus_at = torch.full((b, 1, 1, 1), sqrt_one_minus_alphas[index],device=device)\n",
    "\n",
    "    # current prediction for x_0\n",
    "    pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n",
    "    # direction pointing to x_t\n",
    "    dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n",
    "    noise = sigma_t * torch.randn(x.shape, device=device)\n",
    "    x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n",
    "    return x_prev\n",
    "\n",
    "def sample_x_no_sqrta(x, e_t, index):\n",
    "    # select parameters corresponding to the currently considered timestep\n",
    "    a_t = torch.full((b, 1, 1, 1), alphas[index], device=device)\n",
    "    a_prev = torch.full((b, 1, 1, 1), alphas_prev[index], device=device)\n",
    "    sigma_t = torch.full((b, 1, 1, 1), sigmas[index], device=device)\n",
    "    sqrt_one_minus_at = torch.full((b, 1, 1, 1), sqrt_one_minus_alphas[index],device=device)\n",
    "\n",
    "    # current prediction for x_0\n",
    "    pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n",
    "    # direction pointing to x_t\n",
    "    dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n",
    "    noise = sigma_t * torch.randn(x.shape, device=device)\n",
    "    x_prev = pred_x0 + dir_xt + noise\n",
    "    return x_prev\n",
    "\n",
    "\n",
    "# Initialize conditioning\n",
    "with torch.no_grad():\n",
    "    with autocast('cuda'):\n",
    "        with model.ema_scope():\n",
    "            uc = model.get_learned_conditioning(n * [\"\"]).to(device)\n",
    "            c_i = model.get_learned_conditioning(n * [prompt_i]).to(device)\n",
    "            c_j = model.get_learned_conditioning(n * [prompt_j]).to(device)\n",
    "\n",
    "            uc = dict(c_crossattn=[uc], c_concat=[control_u]).to(device)\n",
    "            c_i = dict(c_crossattn=[c_i], c_concat=[control_i]).to(device)\n",
    "            c_j = dict(c_crossattn=[c_j], c_concat=[control_j]).to(device)\n",
    "\n",
    "sample_path = os.path.join(output_path, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "\n",
    "\n",
    "# Run Diffusion Loop\n",
    "with torch.no_grad():\n",
    "    with autocast('cuda'):\n",
    "        with model.ema_scope():\n",
    "            # Initialize sample x_T to N(0,I)\n",
    "            x = torch.randn((n, ch, h // f, w // f)).to(device)\n",
    "\n",
    "            timesteps = sampler.ddim_timesteps\n",
    "            time_range = np.flip(timesteps)\n",
    "            total_steps = timesteps.shape[0]\n",
    "            print(total_steps)\n",
    "            e_ti = []\n",
    "            e_tj = []\n",
    "            e_t = []\n",
    "            for i, step in enumerate(tqdm(time_range, desc='PLMS Sampler', total=total_steps)):\n",
    "                index = total_steps - i - 1\n",
    "                ts = torch.full((b,), step, device=device, dtype=torch.long)\n",
    "                ts_next = torch.full((b,), time_range[min(i + 1, len(time_range) - 1)], device=device, dtype=torch.long)\n",
    "                \n",
    "                # Compute conditional scores for each concept c_i\n",
    "                x_i, e_ti = p_sample(sampler, x, c_i, ts, index, old_eps=e_ti, t_next=ts_next) \n",
    "                x_j, e_tj = p_sample(sampler, x, c_j, ts, index, old_eps=e_tj, t_next=ts_next)\n",
    "                e_i = e_ti[-1]\n",
    "                e_j = e_tj[-1]\n",
    "\n",
    "                # Compute unconditional score\n",
    "                x_u, e_t = p_sample(sampler, x, uc, ts, index, old_eps=e_t, t_next=ts_next)\n",
    "                e = e_t[-1]\n",
    "                \n",
    "                # Sampling\n",
    "                e_c = (e + w_i * (e_i - e) + w_j * (e_j - e))\n",
    "                \n",
    "                x = sample_x(x, e_c, index)\n",
    "\n",
    "            x = model.decode_first_stage(x)\n",
    "            x = torch.clamp((x + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "            for sample in x:\n",
    "                sample = 255 * rearrange(sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                img = Image.fromarray(sample.astype(np.uint8))\n",
    "                img.save(os.path.join(sample_path, f\"sample_{base_count:05}.png\"))\n",
    "                base_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a9a6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
