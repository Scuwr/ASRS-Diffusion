{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec77a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ac0bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40718bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import extract\n",
    "\n",
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(self, *, timesteps: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Timesteps < 20 => scale > 50 => beta_end > 1 => alphas[-1] < 0 => sqrt_alphas_cumprod[-1] is NaN\n",
    "        assert not timesteps < 20,  f'timsteps must be at least 20'\n",
    "        self.num_timesteps = timesteps\n",
    "        \n",
    "        # Create variance schedule.\n",
    "        scale = 1000 / timesteps\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.02\n",
    "        betas = torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float64)\n",
    "        \n",
    "        print(f'betas:\\n\\t{betas}')\n",
    "        \n",
    "        # Diffusion model constants/buffers. See https://arxiv.org/pdf/2006.11239.pdf\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1,0), value=1.)\n",
    "        \n",
    "        print(f'alphas_cumprod:\\n\\t{alphas_cumprod}')\n",
    "        \n",
    "        # register buffer helper function\n",
    "        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32), persistent=False)\n",
    "\n",
    "        # Register variance schedule related buffers\n",
    "        register_buffer('betas', betas)\n",
    "        register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "\n",
    "        # Buffer for diffusion calculations q(x_t | x_{t-1}) and others\n",
    "        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1.))\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        # Posterior variance:\n",
    "        #   https://github.com/AssemblyAI-Examples/build-your-own-imagen/blob/main/images/posterior_variance.png\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "        register_buffer('posterior_variance', posterior_variance)\n",
    "\n",
    "        # Clipped because posterior variance is 0 at the beginning of the diffusion chain\n",
    "        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min=1e-20)))\n",
    "\n",
    "        # Buffers for calculating the q_posterior mean $\\~{\\mu}$. See\n",
    "        #   https://github.com/oconnoob/minimal_imagen/blob/minimal/images/posterior_mean_coeffs.png\n",
    "        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n",
    "        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n",
    "        \n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if not exists(noise):\n",
    "            noise = lambda: torch.randn_like(x_start)\n",
    "        \n",
    "        noised = (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "        \n",
    "        return noised\n",
    "    \n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "    \n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start + \n",
    "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        \n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "997c9d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "betas:\n",
      "\ttensor([0.0050, 0.0574, 0.1097, 0.1621, 0.2145, 0.2668, 0.3192, 0.3716, 0.4239,\n",
      "        0.4763, 0.5287, 0.5811, 0.6334, 0.6858, 0.7382, 0.7905, 0.8429, 0.8953,\n",
      "        0.9476, 1.0000], dtype=torch.float64)\n",
      "alphas_cumprod:\n",
      "\ttensor([9.9500e-01, 9.3792e-01, 8.3499e-01, 6.9964e-01, 5.4958e-01, 4.0293e-01,\n",
      "        2.7431e-01, 1.7238e-01, 9.9302e-02, 5.2003e-02, 2.4510e-02, 1.0268e-02,\n",
      "        3.7641e-03, 1.1827e-03, 3.0969e-04, 6.4872e-05, 1.0192e-05, 1.0674e-06,\n",
      "        5.5900e-08, 0.0000e+00], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "model = GaussianDiffusion(timesteps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "77438ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from utils import default, cast_tuple\n",
    "from layers import (\n",
    "    Attention,\n",
    "    CrossEmbedLayer,\n",
    "    Downsample,\n",
    "    Residual,\n",
    "    ResnetBlock,\n",
    "    SinusoidalPosEmb,\n",
    "    TransformerBlock,\n",
    "    Upsample, Parallel, Identity\n",
    ")\n",
    "\n",
    "from t5 import get_encoded_dim\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            dim = 128,\n",
    "            dim_mults = (1, 2, 4),\n",
    "            channels = 3,\n",
    "            channels_out = None,\n",
    "            cond_dim = None,\n",
    "            text_embed_dim=get_encoded_dim('t5_small'),\n",
    "            num_resnet_blocks: Union[int, tuple] = 1,\n",
    "            layer_attns: Union[bool, tuple] = True,\n",
    "            layer_cross_attns: Union[bool, tuple] = True,\n",
    "            attn_heads = 8,\n",
    "            lowres_cond = False,\n",
    "            memory_efficient = False,\n",
    "            attend_at_middle = False\n",
    "            ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Save arguments locals to take care of some hyperparameters for cascading DDPM\n",
    "        self._locals = locals()\n",
    "        self._locals.pop('self', None)\n",
    "        self._locals.pop('__class__', None)\n",
    "\n",
    "        # Constants\n",
    "        ATTN_DIM_HEAD = 64  # Dimensionality for attention.\n",
    "        NUM_TIME_TOKENS = 2  # Number of time tokens to use in conditioning tensor\n",
    "        RESNET_GROUPS = 8  # Number of groups in ResNet block GroupNorms\n",
    "\n",
    "        # Model constants\n",
    "        init_conv_to_final_conv_residual = False  # Whether to add skip connection between Unet input and output\n",
    "        final_resnet_block = True  # Whether to add a final resnet block to the output of the Unet\n",
    "\n",
    "        # TIME CONDITIONING\n",
    "\n",
    "        # Double conditioning dimensionality for super-res models due to concatenation of low-res images\n",
    "        cond_dim = default(cond_dim, dim)\n",
    "        time_cond_dim = dim * 4 * (2 if lowres_cond else 1)\n",
    "\n",
    "        # Maps time to time hidden state\n",
    "        self.to_time_hiddens = nn.Sequential(\n",
    "            SinusoidalPosEmb(dim),\n",
    "            nn.Linear(dim, time_cond_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        # Maps time hidden state to time conditioning (non-attention)\n",
    "        self.to_time_cond = nn.Sequential(\n",
    "            nn.Linear(time_cond_dim, time_cond_dim)\n",
    "        )\n",
    "\n",
    "        # Maps time hidden states to time tokens for main conditioning tokens (attention)\n",
    "        self.to_time_tokens = nn.Sequential(\n",
    "            nn.Linear(time_cond_dim, cond_dim * NUM_TIME_TOKENS),\n",
    "            Rearrange('b (r d) -> b r d', r=NUM_TIME_TOKENS)\n",
    "        )\n",
    "        \n",
    "        # TEXT CONDITIONING\n",
    "\n",
    "        self.norm_cond = nn.LayerNorm(cond_dim)\n",
    "\n",
    "        # Projection from text embedding dim to cond_dim\n",
    "        self.text_embed_dim = text_embed_dim\n",
    "        self.text_to_cond = nn.Linear(self.text_embed_dim, cond_dim)\n",
    "\n",
    "        # Create null tokens for classifier-free guidance. See\n",
    "        #   https://www.assemblyai.com/blog/how-imagen-actually-works/#classifier-free-guidance\n",
    "        max_text_len = 256\n",
    "        self.max_text_len = max_text_len\n",
    "        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n",
    "        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n",
    "\n",
    "        # For injecting text information into time conditioning (non-attention)\n",
    "        self.to_text_non_attn_cond = nn.Sequential(\n",
    "            nn.LayerNorm(cond_dim),\n",
    "            nn.Linear(cond_dim, time_cond_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_cond_dim, time_cond_dim)\n",
    "        )\n",
    "\n",
    "        # UNET LAYERS\n",
    "\n",
    "        self.channels = channels\n",
    "        self.channels_out = default(channels_out, channels)\n",
    "\n",
    "        # Initial convolution that brings input images to proper number of channels for the Unet\n",
    "        self.init_conv = CrossEmbedLayer(channels if not lowres_cond else channels * 2,\n",
    "                                         dim_out=dim,\n",
    "                                         kernel_sizes=(3, 7, 15),\n",
    "                                         stride=1)\n",
    "        \n",
    "        # Determine channel numbers for UNet descent/ascent and then zip into in/out pairs\n",
    "        dims = [dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        # Number of resolutions/layers in the UNet\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        # Cast relevant arguments to tuples (with one element for each Unet layer) if a single value rather than tuple\n",
    "        #   was input for the argument\n",
    "        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_resolutions)\n",
    "        resnet_groups = cast_tuple(RESNET_GROUPS, num_resolutions)\n",
    "        layer_attns = cast_tuple(layer_attns, num_resolutions)\n",
    "        layer_cross_attns = cast_tuple(layer_cross_attns, num_resolutions)\n",
    "\n",
    "        # Make sure relevant tuples have one elt for each layer in the UNet (if tuples rather than single values passed\n",
    "        #   in as arguments)\n",
    "        assert all(\n",
    "            [layers == num_resolutions for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n",
    "\n",
    "        # Scale for resnet skip connections\n",
    "        self.skip_connect_scale = 2 ** -0.5\n",
    "\n",
    "        # Downsampling and Upsampling modules of the Unet\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "\n",
    "        # Parameter lists for downsampling and upsampling trajectories\n",
    "        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_cross_attns]\n",
    "        reversed_layer_params = list(map(reversed, layer_params))\n",
    "\n",
    "        # DOWNSAMPLING LAYERS\n",
    "\n",
    "        # Keep track of skip connection channel depths for concatenation later\n",
    "        skip_connect_dims = []\n",
    "\n",
    "        # For each layer in the Unet\n",
    "        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_cross_attn) in enumerate(\n",
    "                zip(in_out, *layer_params)):\n",
    "\n",
    "            is_last = ind == (num_resolutions - 1)\n",
    "\n",
    "            layer_cond_dim = cond_dim if layer_cross_attn else None\n",
    "\n",
    "            # Potentially use Transformer encoder at end of layer\n",
    "            transformer_block_klass = TransformerBlock if layer_attn else Identity\n",
    "\n",
    "            current_dim = dim_in\n",
    "\n",
    "            # Whether to downsample at the beginning of the layer - cuts image spatial size-length\n",
    "            pre_downsample = None\n",
    "            if memory_efficient:\n",
    "                pre_downsample = Downsample(dim_in, dim_out)\n",
    "                current_dim = dim_out\n",
    "\n",
    "            skip_connect_dims.append(current_dim)\n",
    "\n",
    "            # Downsample at the end of the layer if not `pre_downsample`\n",
    "            post_downsample = None\n",
    "            if not memory_efficient:\n",
    "                post_downsample = Downsample(current_dim, dim_out) if not is_last else Parallel(\n",
    "                    nn.Conv2d(dim_in, dim_out, 3, padding=1), nn.Conv2d(dim_in, dim_out, 1))\n",
    "\n",
    "            # Create the layer\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                pre_downsample,\n",
    "                # ResnetBlock that conditions, in addition to time, on the main tokens via cross attention.\n",
    "                ResnetBlock(current_dim,\n",
    "                            current_dim,\n",
    "                            cond_dim=layer_cond_dim,\n",
    "                            time_cond_dim=time_cond_dim,\n",
    "                            groups=groups),\n",
    "                # Sequence of ResnetBlocks that condition only on time\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        ResnetBlock(current_dim,\n",
    "                                    current_dim,\n",
    "                                    time_cond_dim=time_cond_dim,\n",
    "                                    groups=groups\n",
    "                                    )\n",
    "                        for _ in range(layer_num_resnet_blocks)\n",
    "                    ]\n",
    "                ),\n",
    "                # Transformer encoder for multi-headed self attention\n",
    "                transformer_block_klass(dim=current_dim,\n",
    "                                        heads=attn_heads,\n",
    "                                        dim_head=ATTN_DIM_HEAD),\n",
    "                post_downsample,\n",
    "            ]))\n",
    "\n",
    "        # MIDDLE LAYERS\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "\n",
    "        # ResnetBlock that incorporates cross-attention conditioning on main tokens\n",
    "        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim=cond_dim, time_cond_dim=time_cond_dim,\n",
    "                                      groups=resnet_groups[-1])\n",
    "\n",
    "        # Optional residual self-attention\n",
    "        self.mid_attn = EinopsToAndFrom('b c h w', 'b (h w) c',\n",
    "                                        Residual(Attention(mid_dim, heads=attn_heads,\n",
    "                                                           dim_head=ATTN_DIM_HEAD))) if attend_at_middle else None\n",
    "\n",
    "        # ResnetBlock that incorporates cross-attention conditioning on main tokens\n",
    "        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim=cond_dim, time_cond_dim=time_cond_dim,\n",
    "                                      groups=resnet_groups[-1])\n",
    "\n",
    "        # UPSAMPLING LAYERS\n",
    "\n",
    "        # For each layer in the unet\n",
    "        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_cross_attn) in enumerate(\n",
    "                zip(reversed(in_out), *reversed_layer_params)):\n",
    "            is_last = ind == (num_resolutions - 1)\n",
    "            layer_cond_dim = cond_dim if layer_cross_attn else None\n",
    "\n",
    "            # Potentially use Transformer encoder at end of layer\n",
    "            transformer_block_klass = TransformerBlock if layer_attn else Identity\n",
    "\n",
    "            skip_connect_dim = skip_connect_dims.pop()\n",
    "\n",
    "            # Create the layer\n",
    "            self.ups.append(nn.ModuleList([\n",
    "                # Same as `downs` except add channels for skip-connect\n",
    "                ResnetBlock(dim_out + skip_connect_dim,\n",
    "                            dim_out,\n",
    "                            cond_dim=layer_cond_dim,\n",
    "                            time_cond_dim=time_cond_dim,\n",
    "                            groups=groups),\n",
    "                # Same as `downs` except add channels for skip-connect\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        ResnetBlock(dim_out + skip_connect_dim,\n",
    "                                    dim_out,\n",
    "                                    time_cond_dim=time_cond_dim,\n",
    "                                    groups=groups)\n",
    "                        for _ in range(layer_num_resnet_blocks)\n",
    "                    ]),\n",
    "                transformer_block_klass(dim=dim_out,\n",
    "                                        heads=attn_heads,\n",
    "                                        dim_head=ATTN_DIM_HEAD),\n",
    "                # Upscale on the final layer too if memory_efficient to make sure get correct output size\n",
    "                Upsample(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n",
    "            ]))\n",
    "\n",
    "        # Whether to do a final residual from initial conv to the final resnet block out\n",
    "        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n",
    "        final_conv_dim = dim * (2 if init_conv_to_final_conv_residual else 1)\n",
    "\n",
    "        # Final optional resnet block and convolution out\n",
    "        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim=time_cond_dim,\n",
    "                                           groups=resnet_groups[0]) if final_resnet_block else None\n",
    "\n",
    "        # Final convolution to bring to right num channels\n",
    "        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n",
    "        self.final_conv = nn.Conv2d(final_conv_dim_in, self.channels_out, 3,\n",
    "                                    padding=3 // 2)\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            x,\n",
    "            time,\n",
    "            *,\n",
    "            lowres_cond_img = None,\n",
    "            lowres_noise_times = None,\n",
    "            text_embedsr = None,\n",
    "            text_mask = None,\n",
    "            cond_drop_prob: float = 0.\n",
    "            ):\n",
    "        \n",
    "        batch_size, device = x.shape[0], x.device\n",
    "        \n",
    "        assert not (self.lowres_cond and not exists(lowres_cond_img)), \\\n",
    "            'low resolution conditioning image must be present'\n",
    "        assert not (self.lowres_cond and not exists(lowres_noise_times)), \\\n",
    "            'low resolution conditioning noise time must be present'\n",
    "        \n",
    "        # time conditioning\n",
    "        t, time_tokens = self._generate_t_tokens(time, lowres_noise_times)\n",
    "        \n",
    "        # text conditioning\n",
    "        t, c = self._text_condition(text_embeds, batch_size, cond_drop_prob, device, text_mask, t, time_tokens)\n",
    "\n",
    "        # Concatenate low-res image to input if super-resolution Unet\n",
    "        if exists(lowres_cond_img):\n",
    "            x = torch.cat((x, lowres_cond_img), dim=1)\n",
    "\n",
    "        # Initial convolution\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        # Initial convolution clone for residual\n",
    "        if self.init_conv_to_final_conv_residual:\n",
    "            init_conv_residual = x.clone()\n",
    "\n",
    "        # DOWNSAMPLING TRAJECTORY\n",
    "\n",
    "        # To store images for skip connections\n",
    "        hiddens = []\n",
    "\n",
    "        # For every layer in the downwards trajectory\n",
    "        for pre_downsample, init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n",
    "\n",
    "            # Downsample before processing at this resolution if using efficient UNet\n",
    "            if exists(pre_downsample):\n",
    "                x = pre_downsample(x)\n",
    "\n",
    "            # Initial block. Conditions on `c` via cross attention and conditions on `t` via scale-shift.\n",
    "            x = init_block(x, t, c)\n",
    "\n",
    "            # Series of residual blocks that are like `init_block` except they don't condition on `c`.\n",
    "            for resnet_block in resnet_blocks:\n",
    "                x = resnet_block(x, t)\n",
    "                hiddens.append(x)\n",
    "\n",
    "            # Transformer encoder\n",
    "            x = attn_block(x)\n",
    "            hiddens.append(x)\n",
    "\n",
    "            # If not using efficient UNet, downsample after processing at this resolution\n",
    "            if exists(post_downsample):\n",
    "                x = post_downsample(x)\n",
    "\n",
    "        # MIDDLE PASS\n",
    "\n",
    "        # Pass through two ResnetBlocks that condition on `c` and `t`, with a possible residual Attention layer between.\n",
    "        x = self.mid_block1(x, t, c)\n",
    "        if exists(self.mid_attn):\n",
    "            x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t, c)\n",
    "\n",
    "        # UPSAMPLING TRAJECTORY\n",
    "\n",
    "        # Lambda function for skip connections\n",
    "        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim=1)\n",
    "\n",
    "        for init_block, resnet_blocks, attn_block, upsample in self.ups:\n",
    "            # Concatenate the skip connection (post Transformer encoder) from the corresponding layer in the\n",
    "            #   downsampling trajectory and pass through `init_block`, which again conditions on `c` and `t`\n",
    "            x = add_skip_connection(x)\n",
    "            x = init_block(x, t, c)\n",
    "\n",
    "            # For each resnet block, concatenate the corresponding skip connection and then pass through the block.\n",
    "            #   These blocks again condition only on `t`.\n",
    "            for resnet_block in resnet_blocks:\n",
    "                x = add_skip_connection(x)\n",
    "                x = resnet_block(x, t)\n",
    "\n",
    "            # Transformer encoder and upsampling\n",
    "            x = attn_block(x)\n",
    "            x = upsample(x)\n",
    "\n",
    "        # Final skip connect from the initial conv if used\n",
    "        if self.init_conv_to_final_conv_residual:\n",
    "            x = torch.cat((x, init_conv_residual), dim=1)\n",
    "\n",
    "        # Potentially one final residual block\n",
    "        if exists(self.final_res_block):\n",
    "            x = self.final_res_block(x, t)\n",
    "\n",
    "        # Final convolution to get the proper number of channels.\n",
    "        return self.final_conv(x)\n",
    "        \n",
    "    def _generate_t_tokens(\n",
    "            self,\n",
    "            time,\n",
    "            lowres_noise_times\n",
    "            ):\n",
    "\n",
    "        time_hiddens = self.to_time_hiddens(time)\n",
    "        t = self.to_time_cond(time_hiddens)\n",
    "        time_tokens = self.to_time_tokens(time_hiddens)\n",
    "\n",
    "        # If lowres conditioning, add lowres time conditioning to `t` and concat lowres time tokens to `c`\n",
    "        if self.lowres_cond:\n",
    "            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n",
    "            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)\n",
    "            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n",
    "\n",
    "            t = t + lowres_t\n",
    "            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim=-2)\n",
    "\n",
    "        return t, time_tokens\n",
    "    \n",
    "    def _text_condition(\n",
    "            self,\n",
    "            text_embeds,\n",
    "            batch_size,\n",
    "            cond_drop_prob,\n",
    "            device,\n",
    "            text_mask,\n",
    "            t,\n",
    "            time_tokens\n",
    "            ):\n",
    "\n",
    "        text_tokens = None\n",
    "        if exists(text_embeds):\n",
    "\n",
    "            # Project the text embeddings to the conditioning dimension `cond_dim`.\n",
    "            text_tokens = self.text_to_cond(text_embeds)\n",
    "\n",
    "            # Truncate the tokens to have the maximum number of allotted words.\n",
    "            text_tokens = text_tokens[:, :self.max_text_len]\n",
    "\n",
    "            # Pad the text tokens up to self.max_text_len if needed\n",
    "            text_tokens_len = text_tokens.shape[1]\n",
    "            remainder = self.max_text_len - text_tokens_len\n",
    "            if remainder > 0:\n",
    "                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n",
    "\n",
    "            # Prob. mask for clf-free guidance conditional dropout. Tells which elts in the batch to keep. Size (b,).\n",
    "            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device=device)\n",
    "\n",
    "            # Combines T5 and clf-free guidance masks\n",
    "            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n",
    "            if exists(text_mask):\n",
    "                if remainder > 0:\n",
    "                    text_mask = F.pad(text_mask, (0, remainder), value=False)\n",
    "\n",
    "                text_mask = rearrange(text_mask, 'b n -> b n 1')  # (b, self.max_text_len, 1)\n",
    "                text_keep_mask_embed = text_mask & text_keep_mask_embed  # (b, self.max_text_len, 1)\n",
    "\n",
    "            # Creates NULL tensor of size (1, self.max_text_len, cond_dim)\n",
    "            null_text_embed = self.null_text_embed.to(text_tokens.dtype)  # for some reason pytorch AMP not working\n",
    "\n",
    "            # Replaces masked elements with NULL\n",
    "            text_tokens = torch.where(\n",
    "                text_keep_mask_embed,\n",
    "                text_tokens,\n",
    "                null_text_embed\n",
    "            )\n",
    "\n",
    "            # Extra non-attention conditioning by projecting and then summing text embeddings to time (text hiddens)\n",
    "            # Pool the text tokens along the word dimension.\n",
    "            mean_pooled_text_tokens = text_tokens.mean(dim=-2)\n",
    "\n",
    "            # Project to `time_cond_dim`\n",
    "            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)  # (b, cond_dim) -> (b, time_cond_dim)\n",
    "\n",
    "            null_text_hidden = self.null_text_hidden.to(t.dtype)\n",
    "\n",
    "            # Drop relevant conditioning info as demanded by clf-free guidance mask\n",
    "            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n",
    "            text_hiddens = torch.where(\n",
    "                text_keep_mask_hidden,\n",
    "                text_hiddens,\n",
    "                null_text_hidden\n",
    "            )\n",
    "\n",
    "            # Add this conditioning to our `t` tensor\n",
    "            t = t + text_hiddens\n",
    "\n",
    "        # main conditioning tokens `c` - concatenate time/text tokens\n",
    "        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim=-2)\n",
    "\n",
    "        # normalize conditioning tokens\n",
    "        c = self.norm_cond(c)\n",
    "\n",
    "        return t, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eaa22497",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from t5 import t5_encdoe_text\n",
    "\n",
    "class Imagen(nn.Module):\n",
    "    def __init__(self, timesteps):\n",
    "        self.noise_scheduler = GaussianDiffusion(timesteps=timesteps)\n",
    "        self.text_encoder_name = 't5_small'\n",
    "        \n",
    "    def forward(self, images, texts):\n",
    "        times = self.noise_scheduler.sample_random_times(b, device=device)\n",
    "        \n",
    "        text_embeds, text_masks = t5_encode_text(texts, name=self.text_encoder_name)\n",
    "        text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
