{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 101/1000 [00:00<00:04, 200.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not fetch index 118!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 122/1000 [00:09<01:31,  9.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not fetch index 126!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 143/1000 [00:17<02:51,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not fetch index 143!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 147/1000 [00:18<02:54,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not fetch index 144!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 148/1000 [00:19<03:24,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not fetch index 147!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 152/1000 [00:20<03:12,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not fetch index 149!\n",
      "Could not fetch index 150!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 165/1000 [00:23<06:08,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not fetch index 164!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import io\n",
    "import urllib\n",
    "from matplotlib import pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from datasets.utils.file_utils import get_datasets_user_agent\n",
    "import PIL.Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "USER_AGENT = get_datasets_user_agent()\n",
    "\n",
    "dir_path = \"dataset\"\n",
    "images_dir = \"images\"\n",
    "filepath_label = 'filepath'\n",
    "\n",
    "train_num = 1000\n",
    "valid_num = 100\n",
    "\n",
    "def get_img(url, retries=3):\n",
    "    for _ in range(retries):\n",
    "            try:\n",
    "                request = urllib.request.Request(url, data=None, headers={'user-agent': USER_AGENT})\n",
    "                with urllib.request.urlopen(request) as req:\n",
    "                    image = PIL.Image.open(io.BytesIO(req.read()))\n",
    "                break\n",
    "            except:\n",
    "                image = None\n",
    "    \n",
    "    return image\n",
    "\n",
    "def load_data(key, df, num):\n",
    "    count = 0\n",
    "    for i in tqdm(range(num)):\n",
    "        if not df[key][filepath_label][i]:\n",
    "            pass\n",
    "\n",
    "        elif df[key][filepath_label][i] == \"null\":\n",
    "            url = df[key]['image_url'][i]\n",
    "\n",
    "            image = get_img(url)\n",
    "            if not image is None:\n",
    "                filepath = os.path.join(dir_path, images_dir, f'{key}_{i}.jpg')\n",
    "                image.save(filepath)\n",
    "\n",
    "                df[key][filepath_label][i] = filepath\n",
    "            else:\n",
    "                df[key][filepath_label][i] = None\n",
    "                print(f'Could not fetch index {i}!')\n",
    "\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "    print(f'{count} files retrieved in the \"{key}\" split.')\n",
    "    return df\n",
    "\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "    os.makedirs(os.path.join(dir_path, images_dir))\n",
    "\n",
    "    dset = load_dataset(\"conceptual_captions\")\n",
    "\n",
    "    filepaths = [\"null\"] * len(dset['train'])\n",
    "    dset['train'] = dset['train'].add_column(filepath_label, filepaths)\n",
    "\n",
    "    filepaths = [\"null\"] * len(dset['validation'])\n",
    "    dset['validation'] = dset['validation'].add_column(filepath_label, filepaths)\n",
    "\n",
    "    df_train = dset['train'].to_pandas()\n",
    "    df_valid = dset['validation'].to_pandas()\n",
    "\n",
    "    df = {\n",
    "        'train': df_train,\n",
    "        'validation': df_valid\n",
    "          }\n",
    "\n",
    "    with open(os.path.join(dir_path, 'df.pkl'), 'wb') as handle:\n",
    "        pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(dir_path, 'df.pkl'), 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "\n",
    "df = load_data('train', df, train_num)\n",
    "df = load_data('validation', df, valid_num)\n",
    "\n",
    "with open(os.path.join(dir_path, 'df.pkl'), 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#image = transforms.Compose([transforms.ToTensor(), utils.Rescale(64)])(image)\n",
    "#plt.figure(figsize=(8, 8))\n",
    "#plt.imshow(image.permute(1, 2, 0))\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
